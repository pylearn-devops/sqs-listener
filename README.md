# ğŸ¦¾ sqs-fargate-listener

> **A lightweight, Lambda-like SQS listener for Python running on ECS Fargate (or any container).**  
> Handles long-polling, visibility heartbeats, partial batch failure, graceful shutdown, and beautiful colorized logs out of the box.

---

## ğŸš€ Why this exists

When you attach an **SQS** queue to **AWS Lambda**, AWS automatically manages:
- polling the queue,
- invoking your handler,
- deleting successful messages, and
- redriving failures.

On **ECS Fargate**, you donâ€™t get this for free â€” you have to build a poller, manage visibility timeouts, handle retries, and scale your service yourself.

This package brings the same convenience to containers.  
Just decorate a function with `@sqs_listener(...)`, and the library does the rest.

---

## ğŸ§© Install

```bash
pip install sqs-fargate-listener
```

---

# âœ¨ Quick Start

## ğŸ’¡ Processing Messages in `app.py`

When you decorate a function with `@sqs_listener(...)`, youâ€™re telling the library:

> â€œWhenever messages arrive in this queue, call this function with them.â€

The decorator automatically:
- polls the SQS queue,
- extends the visibility timeout while you work,
- invokes your function,
- deletes successful messages,
- and retries or sends failures to the DLQ.

There are **two handler styles** you can use depending on your needs:

---

### ğŸ§© 1. Batch mode (recommended for throughput)

Batch mode receives a *list* of messages (up to `BATCH_SIZE`, default 10) and lets you mark which ones failed.

```python
# app.py
from sqs_fargate_listener import sqs_listener, run_listeners
from sqs_fargate_listener.types import SqsMessage, BatchResult

@sqs_listener(
    queue_url="https://sqs.us-east-1.amazonaws.com/123/orders-queue",
    mode="batch",
    batch_size=10,
)
def handle_orders(messages: list[SqsMessage]) -> BatchResult:
    failed = []

    for msg in messages:
        try:
            # Parse message as JSON (cached after first use)
            data = msg.json
            print(f"ğŸ›’ Processing order {data['order_id']} for {data['customer']}")
            
            # do your logic here (save to DB, call APIs, etc.)
            process_order(data)
        
        except Exception as e:
            print(f"âŒ Failed to process {msg.message_id}: {e}")
            failed.append(msg.receipt_handle)
    
    # Only successful messages will be deleted
    return BatchResult(failed_receipt_handles=failed)

if __name__ == "__main__":
    run_listeners()
```

- âœ… **Advantages:**
  * fewer SQS API calls, better throughput
  * partial batch failure supported
  * automatic retries by SQS DLQ policy

### ğŸª„ 2. Per-message mode (simple boolean handler)

- **Per-message mode gives you one SqsMessage at a time and expects a boolean return:**
  * True â†’ delete the message
  * False or exception â†’ leave in queue for retry

```python
@sqs_listener(
    queue_url="https://sqs.us-east-1.amazonaws.com/123/email-queue",
    mode="per_message",
    worker_threads=4,
)
def handle_email(msg: SqsMessage) -> bool:
    data, err = msg.try_json()
    if err:
        print(f"Invalid JSON: {err}")
        return False

    print(f"ğŸ“§ Sending email to {data['recipient']} ...")
    try:
        send_email(data)
        return True
    except Exception as e:
        print(f"Send failed: {e}")
        return False
```
> **âœ… Simpler to reason about. Best when each message is independent and quick to process.**

### ğŸ“¦ Working with message attributes

- **You can access message attributes attached to the SQS message:**

	```python
	attrs = msg.message_attributes()
	trace_id = attrs.get("trace_id")
	print(f"Processing message trace_id={trace_id}")
	```
- **Make sure your queueâ€™s producer sets attributes when sending messages:**

	```python
	sqs.send_message(
		QueueUrl=queue_url,
		MessageBody=json.dumps({...}),
		MessageAttributes={
			"trace_id": {"DataType": "String", "StringValue": "abc-123"}
		}
	)
	```
 

### ğŸ§© SqsMessage helpers

| Property / Method      | Description                                 |
|------------------------|---------------------------------------------|
| .message_id            | SQS message ID                              |
| .body                  | Raw message body (string)                   |
| .json                  | Cached parsed JSON (raises if invalid)      |
| .try_json()            | Returns (data, error) safely                |
| .message_attributes()  | Simplified dict of SQS MessageAttributes    |
| .receipt_handle        | Internal SQS handle (used for deletion)     |


### ğŸ§° Example: multiple queues in one app

```python
from sqs_fargate_listener import sqs_listener, run_listeners
from sqs_fargate_listener.types import SqsMessage

@sqs_listener(queue_url="https://â€¦/payments-queue", mode="batch")
def process_payments(msgs): ...

@sqs_listener(queue_url="https://â€¦/notifications-queue", mode="per_message")
def send_notifications(msg): ...

if __name__ == "__main__":
    run_listeners()
```
> **Both queues are polled concurrently, each on its own threads.**

### ğŸ›¡ï¸ Error handling & retries
- Uncaught exceptions or return False â†’ message not deleted â†’ retried later.
- After maxReceiveCount (queue setting) â†’ message sent to DLQ.
- You can safely raise any exception; itâ€™s caught by the listener.

For longer jobs, visibility is automatically extended while your handler runs.

### âš™ï¸ How it works

- **Each decorated function spawns one or more polling threads:**
	1.	Long-polls SQS (WaitTimeSeconds=20 by default).
	2.	Receives a batch (up to 10 messages).
	3.	Extends visibility timeout while processing (heartbeat thread).
	4.	Invokes your handler in either batch or per-message mode.
	5.	Deletes successfully processed messages.
	6.	Keeps failed ones for retry / DLQ.
	7.	Gracefully drains on SIGTERM (during ECS scale-in or container stop).

> **Everything runs in-process with no need for AWS Lambda or additional services.**

### ğŸª¶ Features at a glance

| Feature                   | Description                                     |
|---------------------------|-------------------------------------------------|
| Decorator-based API       | Register multiple handlers with \@sqs_listener. |
| Batch & per-message modes | Choose your processing style.                   |
| Automatic long-polling    | Efficient queue reads (WaitTimeSeconds=20).     |
| Visibility heartbeat      | Prevents double processing during long jobs.    |
| Partial batch failure     | Delete successes, leave failed messages.        |
| Graceful shutdown         | Finishes in-flight work on SIGTERM.             |
| Thread-safe concurrency   | Multiple pollers per queue.                     |
| Color-aware logging       | Color in TTY, plain in CloudWatch.              |
| Fully configurable        | Override via decorator args or env vars.        |


---

# ğŸ§  Configuration

Decorator Arguments

```python
@sqs_listener(
  queue_url="...",
  mode="batch",
  wait_time=10,
  batch_size=5,
  visibility_secs=45,
  max_extend=600,
  worker_threads=8,
)
```

## Environment variables

| Variable        | Default | Description                        |
|-----------------|---------|------------------------------------|
| WAIT_TIME       | 20      | SQS long-poll seconds              |
| BATCH_SIZE      | 10      | Max messages per poll              |
| VISIBILITY_SECS | 60      | Initial visibility timeout         |
| MAX_EXTEND      | 900     | Max total visibility extension     |
| WORKER_THREADS  | 4       | Poller threads per listener        |
| IDLE_SLEEP_MAX  | 2.0     | Max random sleep after empty poll  |


> *Note:* Environment variables are overridden by decorator arguments. Precedence: Decorator > Env > Default

---

# ğŸªµ Logging

The package uses colorlog for vivid, structured output thatâ€™s CloudWatch-safe.

Environmental controls:

| Variable         | Default                                           | Description                                  |
|------------------|---------------------------------------------------|----------------------------------------------|
| LOG_LEVEL        | INFO                                              | One of DEBUG, INFO, WARN, ERROR              |
| LOG_USE_COLOR    | 1                                                 | Use ANSI colors (auto-disabled if not a TTY) |
| LOG_FORMAT       | colored pattern                                   | Colored log format                           |
| LOG_PLAIN_FORMAT | [%(levelname)s] %(message)s (%(name)s:%(lineno)d) | Used in non-TTY mode                         |
| LOG_DATEFMT      | %Y-%m-%d %H:%M:%S                                 | Timestamp format                             |


## Examples

**Local Dev**

```bash
LOG_LEVEL=DEBUG python app.py
```

**CloudWatch / ECS:**

```bash
LOG_USE_COLOR=0 python app.py
```

> *Note:* The logger automatically detects non-TTY environments and switches to plain text for clean CloudWatch logs.

---

# ğŸ§° IAM Permissions

The task role (or instance role) needs these actions on your queue:

```json
{
  "Effect": "Allow",
  "Action": [
    "sqs:ReceiveMessage",
    "sqs:DeleteMessage",
    "sqs:ChangeMessageVisibility",
    "sqs:GetQueueAttributes"
  ],
  "Resource": "arn:aws:sqs:REGION:ACCOUNT_ID:QUEUE_NAME"
}
```

## ğŸ§­ Deploying on ECS Fargate

- Build a Docker image with your app and this package installed.

	```dockerfile
	FROM python:3.11-slim
	WORKDIR /app
	COPY . .
	RUN pip install .
	CMD ["python", "app.py"]
	```

- Create a Task Definition:
  * Use your queueâ€™s QUEUE_URL as an environment variable.
  * Add your AWS permissions via IAM task role.
  * Set stopTimeout to â‰¥ 60 seconds for graceful drains.
  
- Run a Service on Fargate:
  * Launch at least one replica.
  * Enable autoscaling based on SQS metrics (ApproximateNumberOfMessagesVisible).

## âš¡ï¸ Autoscaling tips

**Use target tracking on queue depth per task:**

| Metric                                            | Example Target       |
|---------------------------------------------------|----------------------|
| ApproximateNumberOfMessagesVisible / DesiredCount | 10 messages per task |

> *Note:* Scale out when backlog > target, in when < target.

---

# ğŸ§± How it differs from AWS Lambda + SQS


| Capability            | AWS Lambda              | sqs-fargate-listener    |
|-----------------------|-------------------------|-------------------------|
| Managed polling       | âœ…                       | âœ… (inside container)    |
| Pay-per-invocation    | âœ…                       | âŒ (you manage tasks)    |
| Concurrency autoscale | âœ…                       | via ECS autoscaling     |
| Partial batch failure | âœ…                       | âœ…                       |
| Visibility heartbeat  | âœ…                       | âœ…                       |
| Code model            | def handler(event, ctx) | @sqs_listener decorator |
| Local debugging       | Limited                 | âœ…                       |
| Works outside AWS     | âŒ                       | âœ…                       |

---

# ğŸ”’ Graceful shutdown behavior

**When Fargate stops a task, it sends SIGTERM.**

**The listener:**
* Stops fetching new messages.
* Waits for active handlers to finish.
* Extends visibility if needed.
* Exits cleanly.

Set ECS container stopTimeout â‰¥ your typical processing time.

---

# ğŸ§© Advanced patterns

**Multiple queues in one app**

- Decorate multiple functions â€” all run concurrently:

	```python
	@sqs_listener(queue_url="https://â€¦/queueA")
	def handle_a(msg): ...
	
	@sqs_listener(queue_url="https://â€¦/queueB")
	def handle_b(msg): ...
	```

---

# ğŸ§ª Testing locally

```bash
export AWS_ACCESS_KEY_ID=foo
export AWS_SECRET_ACCESS_KEY=bar
export AWS_DEFAULT_REGION=us-east-1
export LOG_LEVEL=DEBUG
python app.py
```

You can use [LocalStack](https://localstack.cloud) or AWSâ€™s [SQS mock](https://docs.aws.amazon.com/cli/latest/reference/sqs/) for local queues.

---

## ğŸ§¾ License

**MIT Â© 2025 AB
Contributions welcome! PRs for async/aiobotocore support, metrics, and batching improvements are encouraged.**
